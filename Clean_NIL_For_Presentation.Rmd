---
title: "Scraping nil.store for data on schools, teams, gender, athletes and products"
output: html_document
date: "2024-06-05"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In light of the NCAA's recent agreement that Power 5 conference schools may be able to directly pay their student athletes for the first time, athlete compensation and Name-Image-Likeness initiatives will only continue to be at the forefront of the college athletics conversation. In addition to contracts with individual businesses, screenprinting companies like https://nil.store offer an opportunity for primarily Division 1 athletes across sports to quickly and directly profit from NIL merchandise. 

The site does not represent the total NIL apparel market for college athletes. Many of the most prominent names in the NIL conversation outside of specifically merchendaise, like former UNC women's basketball player Deja Kelly and LSU gymnast Olivia Dunne, aren't on the website. Particularly for larger conferences (like the SEC, which is only represented on the site by 7 of its 14 member schools), more well-endowed programs (like Duke University, which falls far below its athletic competitors in athlete participation on the site), or more well-established named (like Kelly and Dunne), external sites like nil.store may not be necessary for these programs to wrangle deals for their athletes. 

However, a site like nil.store could also be an opportunity for athletes in less funded sports (like swimming, for example) to enhance their NIL brand without the need for a direct company deal. In this way, composite online stores like the nil.store site can amass a large online marketplace with hundreds of thousands of products. 

With more than 10,000 athletes currently represented across 65 schools (with 3 more soon to be added), there is large potential for business and a huge network of promotional product for these athletes. 

In this project, I will analyze the schools, athletes, genders, and sports more represented on nil.store, as well as what products are most common and how pricing varies between the above categories. Depending on my findings throughout the rest of the week, I can imagine several different stories that my output data could lead me to. 

```{r cars}
library(httr)
library(rvest)
library(dplyr)
library(tidyverse)
```

####Key Findings

##I'm going to add this in when I'm done, with some kind of data viz. 
##ALso, because there's so much code before I get to anything usable, I would love to put a link up here (or multiple links) as like a "click here for this info, click here to go down to all of the data results" and have a chart section at the bottom.

####Part 1

###School-level data

First, I had to scrape the main website page to get the base url for each school's page. I used an adapted version of code from Rob and Aiden to do this. 

```{r}
# URL of the website to scrape
fullurl <- 'https://nil.store/'

# Fetch the webpage content
response <- GET(fullurl)
html_content <- content(response, as = "text")

# Parse the HTML content using rvest
soup <- read_html(html_content)

# Find the specific div containing the school names and URLs
schools_nav <- html_node(soup, css = "div.mega_menu_main#schools-nav")

# Check if the element is found
if (!is.null(schools_nav)) {
  # Find all the list items within this div
  school_list_items <- html_nodes(schools_nav, css = "li.only_link")
  
  # Extract the school names and URLs from the list items
  schools <- lapply(school_list_items, function(item) {
    link <- html_node(item, "a")
    if (!is.null(link)) {
      school_name <- html_text(link, trim = TRUE)
      school_url <- trimws(html_attr(link, "href"))
      list(name = school_name, url = school_url)
    }
  })
  
  # Filter out NULL values
  schools <- Filter(Negate(is.null), schools)
  
  # Create a data frame from the extracted data
  schools_df <- do.call(rbind, lapply(schools, as.data.frame)) %>%
    as.data.frame(stringsAsFactors = FALSE)
  
head(schools_df)
}
```

There were a few issues with that resulting list, including a few schools that didn't load properly because their urls were weird or because their stores weren't active yet, so I removed those here. 

```{r}
#The one URL was wonky, so I had to fix it. 
  schools_df[68, 2] = "https://xavier.nil.store"
  
schools_df_2 <- schools_df %>%
  slice(-59, -25, -18)
```

Using that dataframe, I was able to scrape the "Athletes" header menu for the names of all of the participating athletes and the schools they were attached to. 

```{r}
# Initialize a list to store the results
scraped_results <- list()

# Iterate over each row in the data frame
for (i in 1:nrow(schools_df_2)) {
  name <- schools_df_2$name[i]
  url <- trimws(schools_df_2$url[i])  # Remove any leading/trailing spaces
  webpage <- read_html(url)
  scraped_data <- html_text(html_nodes(webpage, xpath = "//div[contains(@class, 'nested-mobile-menu-heading-container') and contains(@class, 'athlete')]//following-sibling::div[@class='nested-mobile-menu']//li[@class='only_link']//a[@class='color-text']"), trim = TRUE)
  scraped_results[[i]] <- list(name = name, url = url, data = scraped_data)
}

# Combine the list of results into a single data frame
scraped_results_df <- bind_rows(scraped_results)

# Print the results
head(scraped_results_df)
```

This dataframe then allowed me to do some preliminary analysis based on the number of athletes associated with each school. 

```{r}
by_school <- scraped_results_df %>%
  group_by(name) %>%
  summarize(
    athletes = n())
```


<iframe title="More than 10,000 athletes across 65 schools involved in NIL on nil.store" aria-label="Map" id="datawrapper-chart-8CAGH" src="https://datawrapper.dwcdn.net/8CAGH/1/" scrolling="no" frameborder="0" style="width: 0; min-width: 100% !important; border: none;" height="541" data-external="1"></iframe><script type="text/javascript">!function(){"use strict";window.addEventListener("message",(function(a){if(void 0!==a.data["datawrapper-height"]){var e=document.querySelectorAll("iframe");for(var t in a.data["datawrapper-height"])for(var r=0;r<e.length;r++)if(e[r].contentWindow===a.source){var i=a.data["datawrapper-height"][t]+"px";e[r].style.height=i}}}))}();
</script>


Then, to add in some additional information about each school, I extracted a data table from Wikipedia about their Division 1 partner schools. By adapting and joining this information to my existing dataframe, I was able to start to get a bigger picture about the website. 

```{r}
ncaa_url <- "https://en.wikipedia.org/wiki/List_of_NCAA_Division_I_institutions"

division1 <- read_html(ncaa_url) 

ncaa_tables <- division1 %>% 
  html_table(fill = TRUE)

# Display the number of tables extracted
length(ncaa_tables)

# The Wikipedia page had several tables, so I displayed the headers of each to determine which one I wanted â€” the second. 

my_ncaa_table <- ncaa_tables[[2]]

colnames(my_ncaa_table) <- c("School", "Common Name", "Nickname", "City","State", "Type", "Subdivision", "Primary")
colnames(by_school)<- c("Common Name","Number_Athletes")

my_ncaa_table <- my_ncaa_table %>%
  slice(-1)

#Cleaning Before Join
my_ncaa_table[25,2] = "California Baptist"
my_ncaa_table[63,2] = "ETSU"
my_ncaa_table[69,2] = "FGCU"
my_ncaa_table[280,2]= "Mizzou"
my_ncaa_table[304,2]= "Pitt"
my_ncaa_table[179,2]="SDSU"

# Left join
full_school_info <- left_join(by_school, my_ncaa_table, by = "Common Name") 

full_school_info$Primary <- gsub("\\[.*?\\]", "", full_school_info$Primary)
full_school_info$City <- gsub("\\[.*?\\]", "", full_school_info$City)
full_school_info$School <- gsub("\\[.*?\\]", "", full_school_info$School)
full_school_info$Type <- gsub("\\[.*?\\]", "", full_school_info$Type)

head(full_school_info)
```

For example, I was able to break down the participating schools by state, public or private status, and athletic conference. 
```{r}

states <- full_school_info %>%
  select("School","Common Name", "Number_Athletes", "State") 

states_compared <- states %>%
  group_by(State) %>%
  summarise(
    number_of_schools = n(),
    total_athletes = sum(Number_Athletes))

head(states_compared)
```

At the state level, Pennsylvania, Ohio, North Carolina and California were the states with the most schools represented on the website, and with the most athletes represented on the website. Ohio State had the most athletes represented, with almost 600. Kansas had the lowest number of athletes by far, with less than 50. 

Next, I looked at public vs. private schools. I was curious to see this disparity because I wondered if private schools would be less represented in the NIL world because they have more endowment funding to pass on to their athletes, if not directly in NIL deals than with amenities on campus or prominent donors to connect with. Private schools are also less comoonly "big sports schools," I think. 

I was correct that there were far fewer private schools represented on this list, and they also had fewer athletes per school, on average. 

```{r}
pubpriv <- full_school_info %>%
  select("School","Common Name", "Number_Athletes", "Type")

pubpriv_compared <- pubpriv %>%
  group_by(Type) %>%
  summarise(
    number_of_schools = n(),
    total_athletes = sum(Number_Athletes),
    athletes_per_school = total_athletes %/% number_of_schools
    )

print(pubpriv_compared)
```
<iframe title="More than 10,000 athletes across 65 schools involved in NIL on nil.store (Copy)" aria-label="Map" id="datawrapper-chart-yFDjP" src="https://datawrapper.dwcdn.net/yFDjP/1/" scrolling="no" frameborder="0" style="width: 0; min-width: 100% !important; border: none;" height="580" data-external="1"></iframe><script type="text/javascript">!function(){"use strict";window.addEventListener("message",(function(a){if(void 0!==a.data["datawrapper-height"]){var e=document.querySelectorAll("iframe");for(var t in a.data["datawrapper-height"])for(var r=0;r<e.length;r++)if(e[r].contentWindow===a.source){var i=a.data["datawrapper-height"][t]+"px";e[r].style.height=i}}}))}());
</script>



Next, I looked at athletic conferences. I expected that the larger athletic conferences would be on top, and I was correct, but I was surprised that the Southeastern Conference (SEC) wasn't on top. The University of Arkansas is the only SEC school to be in the top 10 on the site for number of athletes. 

I suspect that's because only half of their 14 schools are represented on the site, despite being extremely prominent in the world of college athletics. I wondered if especially large schools like Alabama, like private schools, didn't need the additional NIL support of an external site like nil.store. I'm not quite sure how I would determine that. 

The Big 10 and the ACC were the conferences with the highest athlete participation. 

```{r}
conferences <- full_school_info %>%
  select("School","Common Name", "Number_Athletes", "Subdivision", "Primary")

conferences_compared <- conferences %>%
  group_by(Primary) %>%
  summarise(
    number_of_schools = n(),
    total_athletes = sum(Number_Athletes)
    )

print(conferences_compared)
```

Because the data was there, I also decided to compare by subdivision â€” the Football Bowl Subdision, the Football Championship Subdivision, and non-Football schools. 

I suspected that FBS schools â€” which are the major conference schools â€” would have more participation because sports were more funded and popular at their institutions, and I was correct. However, I was surprised that slightly more non-football schools were on the site than FCS members. 

```{r}
subdivision1 <- full_school_info %>%
  select("School","Common Name", "Number_Athletes", "Subdivision")

subdivision_compared <- subdivision1 %>%
  group_by(Subdivision) %>%
  summarise(
    number_of_schools = n(),
    total_athletes = sum(Number_Athletes)
    )

print(subdivision_compared)
```

####Part 2

###The URL Fight 

At this point, I got stuck in a very convoluted process of trying to determine how to loop any other data from the website. Whoever designed this site did not quite (read:at all) maintain a consistent style in their web design, so it was difficult to extract either the urls for each team's page or for each athlete's page. 

This was a necessary next step because of the drop-down style setup of each page; I had to go through each level (first school, then team, then athlete, or first school, then athlete) to locate information about each athlete's products, or information about the specific sports more represented in the NIL market. 

First, using the urls that we pulled in step one, I (semi-accidentally) scraped every single menu header on every single school's page, and its corresponding url ending.It doesn't have a particularly accurate name because that's not what I thought it was going to do, but we move along!

```{r}
# Initialize a list to store the results
gender_sport <- list()

for (i in 1:nrow(schools_df_2)) {
  name <- schools_df_2$name[i]
  url <- trimws(schools_df_2$url[i])  # Remove any leading/trailing spaces
  webpage <- read_html(url)
  scraped_data <- html_text(html_nodes(webpage, xpath= "//div[@class='nested-mobile-menu']//li//a[@class='color-text']/@href"))
  gender_sport[[i]] <- list(name = name, url = url, data = scraped_data)
}

# Combine the list of results into a single data frame
gender_sport_df <- bind_rows(gender_sport)
  
# Print the results
head(gender_sport_df)
```
My next task was combining all of the base urls with the sub-menu headings, to produce a list of web values that I could scrape further. However, this process was slightly complicated by the fact that at each school, not every sub-menu attached to the full base url. In the case of the University of Arkansas, shown in the header above, it attached no problem if you removed either the ending / of the base url or the beginning / of the sub-menu heading. 

However, some schools had longer base urls that didn't translate to their sub-menu urls. For example, these urls from Baylor didn't need the "/pages/baylor" part of the base url. 
```{r}
print(gender_sport_df[590:595,])

```
 With that in mind, I joined the two pieces like this: 
 
```{r}
# Load the dplyr package
library(dplyr)

# Define a function to extract the desired portion of the URL
extract_url <- function(url, data) {
  parts <- unlist(strsplit(url, "/"))
  combined_url <- paste(parts[1:3], collapse = "/")
  return(paste0(combined_url, data))
}

# Apply the function to the 'url' column using mutate and rowwise
gender_sport_df <- gender_sport_df %>%
  rowwise() %>%
  mutate(combined_url = extract_url(url, data))

head(gender_sport_df)
```
This produced an additional column with all the the team page sub-urls, the athlete page sub-urls, and a bunch of other random ones that I then had to remove by a long and tedious process.
 
```{r}
gender_sport_df <- gender_sport_df %>% filter(
    !grepl("shop-by-sport$", combined_url) & 
      !grepl("jerseys", combined_url) & 
      !grepl("jersey", combined_url) & 
      !grepl("sports", combined_url) & 
      !grepl("onit-trading-card", combined_url) & 
      !grepl("exclusive-drops", combined_url) & 
      !grepl("postseason-apparel", combined_url) & 
      !grepl("limited-releases", combined_url) & 
      !grepl("limited-release", combined_url) & 
      !grepl("trading-cards", combined_url) &
      !grepl("bulldog-initiative", combined_url) &
      !grepl("legacy", combined_url) & 
      !grepl("tees", combined_url) &
      !grepl("dawg-mentality-collection", combined_url) &
      !grepl("big-red-helmet-collection", combined_url) &
      !grepl("future", combined_url) &
      !grepl("releases", combined_url) &
      !grepl("volleyball-day", combined_url) &
      !grepl("jackets", combined_url) &
      !grepl("locker-room", combined_url) &
      !grepl("legacy", combined_url) &
      !grepl("merch", combined_url) &
      !grepl("coming-soon", combined_url) &
      !grepl("https://nil.shop/search", combined_url) &
      !grepl("collective", combined_url) &
      !grepl("search", combined_url) &
      !grepl("champions", combined_url) &
       !grepl("shirseys", combined_url) &
       !grepl("https://nil.store/pages/pitt/", combined_url) &
       !grepl("champion", combined_url) &
       !grepl("four", combined_url) &
       !grepl("eight", combined_url) &
      !grepl("sixteen", combined_url) &
      !grepl("stop", combined_url) &
      !grepl("apparel", combined_url) &
      !grepl("raglan", combined_url) &
      !grepl("https://nil.store/pages/baylor/pages/baylor", combined_url) &
      url!= combined_url &
      substr(combined_url, nchar(combined_url), nchar(combined_url)) != "/" &
      substr(combined_url, nchar(combined_url), nchar(combined_url)) != "#"
      )
```

I was then left with the problem of how to split the remaining values in the dataframe into sport pages and athlete pages, as the code for scraping each of those would be different things. I was able to scrape the sport page urls specifically into a separate dataframe with this code. 

First for men's sports: 
```{r}
mens_sports_df <- list()

for (i in 1:nrow(schools_df_2)) {
  name <- schools_df_2$name[i]
  url <- trimws(schools_df_2$url[i])
  
  tryCatch({
    html <- read_html(url)
    mens_sports <- html %>%
      html_nodes(".nested-mobile-menu li:contains(\"Men's\") li.only_link a.color-text") %>%
      html_attr("href")
    
    if (length(mens_sports) > 0) {
      mens_sports_df[[name]] <- mens_sports
    } else {
      mens_sports_df[[name]] <- NA
    }
  }, error = function(e) {
    mens_sports_df[[name]] <- NA
    warning(paste("Error occurred for", name, ":", e$message))
  })
}

# Unpack the list column and convert to a DataFrame
mens_sports_df <- mens_sports_df %>%
  enframe(name = "name", value = "mens_sports") %>%
  unnest(mens_sports)

# Join with the schools_df_2 DataFrame to include the base URL
mens_sports_df <- mens_sports_df %>%
  left_join(schools_df_2, by = "name") %>%
  mutate(gender = "Men's")

# Print the resulting DataFrame
head(mens_sports_df)
```

And then for Women's:

```{r}
womens_sports_df <- list()

for (i in 1:nrow(schools_df_2)) {
  name <- schools_df_2$name[i]
  url <- trimws(schools_df_2$url[i])
  
  tryCatch({
    html <- read_html(url)
    womens_sports <- html %>%
      html_nodes(".nested-mobile-menu li:contains(\"Women's\") li.only_link a.color-text") %>%
      html_attr("href")
    
    if (length(mens_sports) > 0) {
      womens_sports_df[[name]] <- womens_sports
    } else {
      womens_sports_df[[name]] <- NA
    }
  }, error = function(e) {
    womens_sports_df[[name]] <- NA
    warning(paste("Error occurred for", name, ":", e$message))
  })
}

# Unpack the list column and convert to a DataFrame
womens_sports_df <- womens_sports_df %>%
  enframe(name = "name", value = "womens_sports") %>%
  unnest(womens_sports) 

# Join with the schools_df_2 DataFrame to include the base URL
womens_sports_df <- womens_sports_df %>%
  left_join(schools_df_2, by = "name") %>%
  mutate(gender = "Women's")

# Print the resulting DataFrame
head(womens_sports_df)
```

Then, I joined them, changing the column names for consistency and adding a column to clarify Men's or Women's: 

```{r}
colnames(mens_sports_df) <- c("school", "sport_url", "main_url", "gender")
colnames(womens_sports_df) <- c("school", "sport_url", "main_url", "gender")


# Join the men's and women's sports DataFrames and add a gender column
all_sports_df <- bind_rows(
  mens_sports_df %>% mutate(gender = "Men's"),
  womens_sports_df %>% mutate(gender = "Women's")
)

# Print the resulting DataFrame
head(all_sports_df)
```
Then, similarly to what I did with my mega database, the urls to create a new column with the full urls for each team's page. 

```{r}
# Load the dplyr package
library(dplyr)

# Define a function to extract the desired portion of the URL
extract_url <- function(main_url, sport_url) {
  parts <- unlist(strsplit(main_url, "/"))
  combined_url <- paste(parts[1:3], collapse = "/")
  return(paste0(combined_url, sport_url))
}

# Apply the function to the 'url' column using mutate and rowwise
all_sport_pages <- all_sports_df %>%
  rowwise() %>%
  mutate(combined_url = extract_url(main_url, sport_url))

# Print the updated dataframe
head(all_sport_pages)
```

(DO SOME ANALYSIS WITH TEAM AND GENDER HERE) â€” DATA VIZ

Using the dataframe with the full list of team urls, I created a list of athlete urls with all of the values from the gender_sport_df table that were NOT in the all_sport_pages table. 

```{r}
# Create the third dataframe athlete_urls
athlete_urls <- anti_join(gender_sport_df, all_sport_pages, by = "combined_url")

# View the resulting dataframe
head(athlete_urls)
```

There were still some lingering non-athlete urls, so I removed them manually. 

```{r}
# Define the row indices to be removed
rows_to_remove <- c(9907, 9662, 9574, 9516, 9463, 9384, 9328:9330, 9301, 9197, 
                    9138, 9088, 9034, 8490, 8429, 8367, 8297, 8215, 8167, 8106, 
                    7004, 5293, 5292, 5265, 5182, 3615, 3614, 3545, 3422:3424, 
                    3126:3128, 2740, 2741, 2492, 2209, 2210, 2179:2181, 2136:2141, 
                    2097, 2006, 1877, 1818, 1817, 1816, 1767, 1715, 1662, 1242, 
                    645, 578, 516, 453)

# Remove the specified rows from the dataframe
athlete_urls <- athlete_urls[-rows_to_remove, ]

# View the resulting dataframe
head(athlete_urls)
```

SCRAPE PRODUCT PAGES TOMORROW YEET! 

