---
title: "Hannah R. Clean NIL Day 2"
output: html_document
date: "2024-06-03"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Loading packages}
library(httr)
library(rvest)
library(dplyr)
```
This project focuses on Name, Image, Likeness opportunities for Division 1 college athletes, across sports, but specifically focused on branded merchandise available through the online site https://nil.store. 

While the site does not represent the total NIL apparel market for college athletes, it boasts over 10,000 registered athletes across 65 schools. (INSERT CONTEXT â€” other sites, not all athletes represented, wealthier/private/better funded athletic programs potentially don't need external sites like nil.store to assist their athletes with NIL deals, etc)

I used Aidan and Rob's code for establishing a list of schools and their URL's on the nil.store site. This output will be used as the basis of the rest of my scraping and analysis. 

```{r}
# URL of the website to scrape
fullurl <- 'https://nil.store/'

# Fetch the webpage content
response <- GET(fullurl)
html_content <- content(response, as = "text")

# Parse the HTML content using rvest
soup <- read_html(html_content)

# Find the specific div containing the school names and URLs
schools_nav <- html_node(soup, css = "div.mega_menu_main#schools-nav")

# Check if the element is found
if (!is.null(schools_nav)) {
  # Find all the list items within this div
  school_list_items <- html_nodes(schools_nav, css = "li.only_link")
  
  # Extract the school names and URLs from the list items
  schools <- lapply(school_list_items, function(item) {
    link <- html_node(item, "a")
    if (!is.null(link)) {
      school_name <- html_text(link, trim = TRUE)
      school_url <- trimws(html_attr(link, "href"))
      list(name = school_name, url = school_url)
    }
  })
  
  # Filter out NULL values
  schools <- Filter(Negate(is.null), schools)
  
  # Create a data frame from the extracted data
  schools_df <- do.call(rbind, lapply(schools, as.data.frame)) %>%
    as.data.frame(stringsAsFactors = FALSE)
  
  print(schools_df)
}

#The one URL was wonky, so I had to fix it. 
  schools_df[68, 2] = "https://xavier.nil.store"
  
print(schools_df$url)
```

Next, I used that base (and some other base code from Aidan and Rob as a starting point) to attach the names of all of the athletes to their respective schools.

```{r}
#First, the Illinois site wasn't loading, so I had to exclude it while I was working on this phase so my code would load. 

schools_df_2 <- schools_df %>%
  slice(-25)

# Initialize a list to store the results
scraped_results <- list()

# Iterate over each row in the data frame
for (i in 1:nrow(schools_df_2)) {
  name <- schools_df_2$name[i]
  url <- trimws(schools_df_2$url[i])  # Remove any leading/trailing spaces
  webpage <- read_html(url)
  scraped_data <- html_text(html_nodes(webpage, xpath = "//div[contains(@class, 'nested-mobile-menu-heading-container') and contains(@class, 'athlete')]//following-sibling::div[@class='nested-mobile-menu']//li[@class='only_link']//a[@class='color-text']"), trim = TRUE)
  scraped_results[[i]] <- list(name = name, url = url, data = scraped_data)
}

# Combine the list of results into a single data frame
scraped_results_df <- bind_rows(scraped_results)

# Print the results
print(scraped_results_df)
```


Grouping that full dataframe by school, we're left with the number of athletes participating at each school.

```{r}
by_school <- scraped_results_df %>%
  group_by(name) %>%
  summarize(
    athletes = n())
```

Taken together, these dataframes show that nil.store is living up to their advertisement of more than 10,000 athletes signed to their site, showing 10,270 total distinct athletes across 65 schools with functional sites. 

Because I got stuck trying to do this by sport and gender, I focused on getting other comparison data for today. First, I scraped a table of Division 1 School info from Wikipedia.

```{r}
ncaa_url <- "https://en.wikipedia.org/wiki/List_of_NCAA_Division_I_institutions"

division1 <- read_html(ncaa_url) 

ncaa_tables <- division1 %>% 
  html_table(fill = TRUE)

# Display the number of tables extracted
length(ncaa_tables)

# Display the first few rows of each table to find the one you need
for (i in 1:length(ncaa_tables)) {
  cat("Table", i, ":\n")
  print(head(ncaa_tables[[i]]))
  cat("\n\n")
}

my_ncaa_table <- ncaa_tables[[2]]
```

I cleaned the table, and then joined it with my existing information from nil.store. 

```{r}
colnames(my_ncaa_table) <- c("School", "Common Name", "Nickname", "City","State", "Type", "Subdivision", "Primary")
colnames(by_school)<- c("Common Name","Number_Athletes")

my_ncaa_table <- my_ncaa_table %>%
  slice(-1)

#Cleaning Before Join

my_ncaa_table[25,2] = "California Baptist"
my_ncaa_table[63,2] = "ETSU"
my_ncaa_table[69,2] = "FGCU"
my_ncaa_table[280,2]= "Mizzou"
my_ncaa_table[304,2]= "Pitt"
my_ncaa_table[179,2]="SDSU"

# Install and load the dplyr package
library(dplyr)

# Left join
full_school_info <- left_join(by_school, my_ncaa_table, by = "Common Name") 

full_school_info$Primary <- gsub("\\[.*?\\]", "", full_school_info$Primary)
full_school_info$City <- gsub("\\[.*?\\]", "", full_school_info$City)
full_school_info$School <- gsub("\\[.*?\\]", "", full_school_info$School)
full_school_info$Type <- gsub("\\[.*?\\]", "", full_school_info$Type)

print(full_school_info)
```

Then, I produced several additional dataframes to compare athlete participation by state, type of school (public/private), subdivision, and conference. None of this information was particularly surprising, but some of the biggest disparities were interesting to see!

```{r}
#STATE LEVEL
states <- full_school_info %>%
  select("School","Common Name", "Number_Athletes", "State") 

states_compared <- states %>%
  group_by(State) %>%
  summarise(
    number_of_schools = n(),
    total_athletes = sum(Number_Athletes))

#PUBLIC V. PRIVATE
pubpriv <- full_school_info %>%
  select("School","Common Name", "Number_Athletes", "Type")

pubpriv_compared <- pubpriv %>%
  group_by(Type) %>%
  summarise(
    number_of_schools = n(),
    total_athletes = sum(Number_Athletes)
    )

#LEVEL OF DIVISION 1

subdivision1 <- full_school_info %>%
  select("School","Common Name", "Number_Athletes", "Subdivision")

subdivision_compared <- subdivision1 %>%
  group_by(Subdivision) %>%
  summarise(
    number_of_schools = n(),
    total_athletes = sum(Number_Athletes)
    )

#CONFERENCE

conferences <- full_school_info %>%
  select("School","Common Name", "Number_Athletes", "Subdivision", "Primary")

conferences_compared <- conferences %>%
  group_by(Primary) %>%
  summarise(
    number_of_schools = n(),
    total_athletes = sum(Number_Athletes)
    )

conferences_and_sub <- conferences %>%
  group_by(Primary, Subdivision) %>%
  summarise(
    number_of_schools = n(),
    total_athletes = sum(Number_Athletes)
    )

print(states_compared)
print(pubpriv_compared)
print(subdivision_compared)
print(conferences_compared)
print(conferences_and_sub)
```


Additionally, I think this will more come in hand later for visuals, but I have a dataframe with links to each school's logo. 

```{r}
# Initialize a list to store the results
Logos <- list()

for (i in 1:nrow(schools_df_2)) {
  name <- schools_df_2$name[i]
  url <- trimws(schools_df_2$url[i])
  webpage <- read_html("https://nil.store")
  school <- html_text(html_nodes(webpage, xpath = "//li[@class='only_link']/a[@class='color-text']"))
  logo <- html_attr(html_nodes(webpage, xpath = "//li[@class='only_link']/a/img"), "src")
}

# Create a data frame with the extracted data
Logos <- data.frame(School = school, LogoLink = logo) %>%
  slice(1:68) 

Logos$LogoLink <- paste("https:", Logos$LogoLink, sep = "")
```